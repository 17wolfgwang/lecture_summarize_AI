When calling a function, the function call gets pushed onto the call stack, which is short-term memory used for executing code, and as the name implies, it's based on the stack data structure, which means last in first out. To implement algorithms, you often have to compare two values, which you can do with operators like greater than or equality, and logical expressions such as AND, OR and NOT. Expressions like these are simple: they can be true or false, which are the possible values of the Boolean data type and allow you to write conditional statements: If some condition is true, do this, else do that.

Booleans can also be used to loop over certain parts of code. One way is with a while loop: While this condition is true, this code will execute. Another way is a for loop, which can iterate over every element inside a data structure like an array, but can also loop for a specific number of iterations, by setting a starting value, incrementing it after each iteration, and setting an upper bound with a condition. Functions can also call themselves, which is known as "recursion". This is useful when a problem can be broken down into smaller identical problems, such as calculating 5 factorial, which is just 5 times 4 factorial, which is just 4 times 3 factorial and so on. But, by default, a recursive function will just keep on calling itself forever. This means that it keeps pushing more function calls onto the call stack, until the stack memory is exceeded in a "stack overflow".

To stop this, you have to add a base condition to a recursive function, which defines when to stop. Only then will the function calls be executed without crashing your PC.

Recursion is cool, but it can be pretty expensive time and spacewise. So, to minimize the amount of computations needed, past results can be saved in a cache, so if they come up again, the computer doesn't have to recompute them from scratch. This is called "memoization".

Speaking of performance, to judge how good an algorithm is, you can look at time and space complexity, so how much time or space is required to run it. This is measured in Big O notation, which describes the relationship between growth of input size and number of operations needed to execute the algorithm. For example, adding 1 to every number inside an array is O(n), because the number of operations increases in a linear way as the array grows.

What's relevant is not the exact number of operations, but rather the trend as the the input size goes to infinity. You see, something like n! grows way faster than any linear function ever could, so as long as the time complexity is some kind of linear relation, it's simplified down to O(n), and the same thing goes for any other group. When writing algorithms, there's some general approaches: For example, when searching an item in a list, a brute force approach would be to simply check every item until we find the target, but a more sophisticated approach would be divide and conquer. For example, in binary search, you cut the problem in half each time by checking the middle element of a list and seeing on which side the target is until you land on the target.

Now, when solving a problem with code, there's always multiple ways to achieve the same result, which are called programming paradigms. Let's try to find the sum of all elements in this list. "Declarative programming", describes what the code does, but not how exactly the computer should do it, whereas "Imperative programming" explicitly describes how the computer should achieve a result with detailed instructions. An extension of imperative programming is object-oriented programming, where you can define classes as blueprints for objects, which are single units consisting of data in the form of properties and behaviours in the form of methods.

To code a call, you begin by defining the properties as variables, and the methods as functions. After encapsulating properties and methods in a class, you can instantiate an object and use the dot notation to work with its properties and methods. Classes make it easy to organize and reuse code, because you can define subclasses that inherit properties and behaviours of a superclass, but can also extend and override them. For example, a RubberDuck subclass might implement quack() with a squeak(), instead. As a result, rubber rucks can be treated as objects of the Duck class, but behave differently when quack() is called, which is the concept of "polymorphism".

But for some problems, none of these traditional paradigms will work. Let's say you want to make a computer recognize which of these images is a bee. The problem is, you can't really describe what a bee looks like with code.

This is where machine learning comes in, aka teaching a computer to do a task without explicitly programming it to do that task. First you need a lot of data which you split into training and test data. Next you choose an algorithm that can change its parameters over time, for example a neural network, where the weights can be updated to achieve a different result. By feeding lots and lots of training data into this algorithm you can build a model, whose accuracy you can then check with the test data.

If it's not quite right, the model can improve over time by comparing the output to what it should have been, capturing the difference in an error function, and tweaking its parameters to minimize the difference. But no matter how futuristic, bleeding-edge, blazingly fast and optimized it is, if you want people to actually use the application you wrote, you should probably know about the internet. It's a network of computers from all around the globe connected by wires. Like, literally, the internet is a bunch of thicc cables that run at the bottom of the ocean along with facilities like Internet Service Providers that connect you to your destination. These computers communicate with the Internet Protocol Suite. Every computer on the network has a unique IP address. Two computers can then transfer data with the transmission control protocol. It breaks messages into a bunch of packets, sends them through a network of wires, before the receiving end puts the message back together. If you have a poor internet connection, you might have experienced "packet loss", which is just if some these packets get lost along the way. If the internet is the hardware, then the web is the software, which you can use with a browser. Every page on the web has a URL. When you type it into your browser, it looks up the IP address of the server hosting this website with the domain name system, which is like a dictionary mapping domain names to IP addresses of actual servers.